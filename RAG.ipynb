{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "de6abe6d-03b0-4335-b388-fefad3e0172d",
      "metadata": {
        "id": "de6abe6d-03b0-4335-b388-fefad3e0172d"
      },
      "source": [
        "# Retrieval augmented generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a57245-e4fe-4b22-9d70-b419523b11a2",
      "metadata": {
        "id": "e5a57245-e4fe-4b22-9d70-b419523b11a2"
      },
      "source": [
        "## Loading Documents\n",
        "A first step in RAG is to load document. You need a loader that supports the document type you are interested in. We use in this example Langchain, because it includes a collection of 60+ libraries for multiple types of documents and formats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03dc9db-b35b-40d7-b94f-013c6cc41d50",
      "metadata": {
        "id": "c03dc9db-b35b-40d7-b94f-013c6cc41d50"
      },
      "source": [
        "A first example with the `PyPDFLoader` library. Pdf support is direct and a single command is enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "926d4fa1-897d-476a-a16a-caea89e1337a",
      "metadata": {
        "id": "926d4fa1-897d-476a-a16a-caea89e1337a",
        "outputId": "b277ac18-b09f-4e69-995a-151337ed0663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.14 (from langchain-community)\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.24.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.24.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.14 langchain-community-0.3.14 langchain-core-0.3.29 marshmallow-3.24.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# For this loading Documents part, you may need these packages installed\n",
        "\n",
        "!pip install langchain\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8ba47b07-c15a-407e-abaf-06e13b147f3c",
      "metadata": {
        "id": "8ba47b07-c15a-407e-abaf-06e13b147f3c",
        "outputId": "caeccc62-6cb1-4145-f702-3d1222941c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File path docs/War-of-the-Worlds.pdf is not a valid file or url",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-00c261c0e696>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docs/War-of-the-Worlds.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34m\"pypdf package not found, please install it with `pip install pypdf`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         self.parser = PyPDFParser(\n\u001b[1;32m    243\u001b[0m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path docs/War-of-the-Worlds.pdf is not a valid file or url"
          ]
        }
      ],
      "source": [
        "import warnings # optional, disabling warnings about versions and others\n",
        "warnings.filterwarnings('ignore') # optional, disabling warnings about versions and others\n",
        "\n",
        "!pip install pypdf\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"https://github.com/behreth/GenAI-Foundations/docs/War-of-the-Worlds.pdfWar-of-the-Worlds.pdf\")\n",
        "book = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0155ef27-face-4f42-8359-b36b8aaf3eec",
      "metadata": {
        "id": "0155ef27-face-4f42-8359-b36b8aaf3eec",
        "outputId": "2af2c2a4-5637-46d8-e6f7-9ff24c458516"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How long is the document we loaded?\n",
        "len(book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be420767-f472-40a2-a239-dc61747a697a",
      "metadata": {
        "id": "be420767-f472-40a2-a239-dc61747a697a",
        "outputId": "4444f51d-3319-4928-c42f-3088ae8cea9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "darkness were Ottershaw and Chertsey and a ll their hundreds of people, sleeping in \n",
            "peace.  \n",
            "   He was full of speculation that night a bout the condition of Mars, and scoffed at the \n",
            "vulgar idea of its having in- habitants w ho were signalling us. His idea was that \n",
            "meteorites might be falling in a heavy shower upon the planet, or that a huge volcanic \n",
            "explosion was in progress. He pointed out to me how unlikely it was that organic \n",
            "evolution had taken the same direction in the two adjacent pl\n"
          ]
        }
      ],
      "source": [
        "#Looking at a small extract, one page, and a few hundred characters in that page\n",
        "page = book[5]\n",
        "print(page.page_content[0:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78963c00-21cd-45fc-9b11-d4c8b695e917",
      "metadata": {
        "id": "78963c00-21cd-45fc-9b11-d4c8b695e917",
        "outputId": "eb7c1d82-d9a9-45d1-af9c-e6f88d5e6ef6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 5}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Which page is it, from which document?\n",
        "page.metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a16ab2-a33a-4dc3-a58d-8434d549e3a5",
      "metadata": {
        "id": "85a16ab2-a33a-4dc3-a58d-8434d549e3a5"
      },
      "source": [
        "A second example with a Youtube video. There is a little more work here. The yt_dlp library will need options to know what audio format to download (we won't care much about the video part). Here we use m4a, at 192 kbps. Then the ffmpeg and ffprobe programs will isolate and stream the audio part. We will then use the OpenAI whisper library to covnert the audio into text (speech-to-text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92996683-f7ae-40bf-8a84-c1fbdbf33e3f",
      "metadata": {
        "id": "92996683-f7ae-40bf-8a84-c1fbdbf33e3f",
        "outputId": "2d7eb1a2-bd65-44ec-d75b-c10132516727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=2vkJ7v0x-Fs\n",
            "[youtube] 2vkJ7v0x-Fs: Downloading webpage\n",
            "[youtube] 2vkJ7v0x-Fs: Downloading ios player API JSON\n",
            "[youtube] 2vkJ7v0x-Fs: Downloading web creator player API JSON\n",
            "[youtube] 2vkJ7v0x-Fs: Downloading player 3c3d6e4f\n",
            "[youtube] 2vkJ7v0x-Fs: Downloading m3u8 information\n",
            "[info] 2vkJ7v0x-Fs: Downloading 1 format(s): 251\n",
            "[download] Destination: docs/youtube/Big Data Architectures.webm\n",
            "[download] 100% of   22.03MiB in 00:00:02 at 10.69MiB/s    \n",
            "[ExtractAudio] Destination: docs/youtube/Big Data Architectures.m4a\n",
            "Deleting original file docs/youtube/Big Data Architectures.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade --no-deps --force-reinstall yt_dlp\n",
        "#! pip install pydub\n",
        "#!pip install ffmpeg\n",
        "#!pip install ffprobe\n",
        "#!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os\n",
        "import whisper\n",
        "from yt_dlp import YoutubeDL\n",
        "\n",
        "# Step 1: Set up the download options\n",
        "url = \"https://www.youtube.com/watch?v=2vkJ7v0x-Fs\"\n",
        "save_dir = \"docs/youtube/\"\n",
        "output_template = os.path.join(save_dir, '%(title)s.%(ext)s')\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'outtmpl': output_template,  # Save the file to the specified directory with a title-based name\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'm4a',  # You can change this to mp3 if you prefer\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    'ffmpeg_location': '/opt/homebrew/bin/ffmpeg',  # Specify the location of ffmpeg\n",
        "}\n",
        "\n",
        "\n",
        "# Step 2: Download the audio from the YouTube video\n",
        "with YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url])\n",
        "\n",
        "# Step 3: Find the downloaded file\n",
        "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
        "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
        "\n",
        "# Step 4: Load the Whisper model\n",
        "model = whisper.load_model(\"base\")  # You can choose 'tiny', 'base', 'small', 'medium', or 'large'\n",
        "\n",
        "# Step 5: Transcribe the audio file\n",
        "result = model.transcribe(downloaded_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2e169a-66bc-4f9e-bacb-05537633d599",
      "metadata": {
        "id": "be2e169a-66bc-4f9e-bacb-05537633d599",
        "outputId": "7a358341-d453-4fbb-91c8-37d41c65c741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript saved to docs/youtube/transcript.txt\n"
          ]
        }
      ],
      "source": [
        "# Adding metadata to the transcript, and saving the transcript to a file so we can use it outside of this program.\n",
        "class Document:\n",
        "    def __init__(self, source, text, metadata=None):\n",
        "        self.source = source\n",
        "        self.page_content = text\n",
        "        self.metadata = metadata or {}\n",
        "\n",
        "# Wrap the transcription result in the Document class with metadata\n",
        "document = Document(\n",
        "    source=downloaded_file_path,\n",
        "    text=result['text'],\n",
        "    metadata={\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
        ")\n",
        "#Save the transcript to a text file\n",
        "transcript_file_path = os.path.join(save_dir, 'transcript.txt')\n",
        "with open(transcript_file_path, 'w') as f:\n",
        "    f.write(result['text'])\n",
        "\n",
        "print(f\"Transcript saved to {transcript_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e4c04b-8eb6-411a-9876-181447c55824",
      "metadata": {
        "id": "37e4c04b-8eb6-411a-9876-181447c55824",
        "outputId": "03b1f8be-8d68-403a-cef4-ee7d2bee64cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32857"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many characters in this transcript file?\n",
        "len(document.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6311ecd-14e7-43d0-ae65-9302e22a9b88",
      "metadata": {
        "id": "d6311ecd-14e7-43d0-ae65-9302e22a9b88",
        "outputId": "21d1ed3c-613b-41bf-c4af-95cc9c10bb77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " In lesson four, we will go deeper into architectures for big data, and we will take a closer look at some of the most popular big data management systems. First, we're going to look at how the big data management system framework looks, and explore the commonalities that pretty much all the big data systems have, as well as some of the key differences between no SQL, MPP, and Hadoop. Next, we're going to take a deep dive into the Hadoop data management system. You will see how we both store dat\n"
          ]
        }
      ],
      "source": [
        "# Print the first 500 characters of the transcript\n",
        "print(document.page_content[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641d8b7b-a005-4431-9238-4647e4c89510",
      "metadata": {
        "id": "641d8b7b-a005-4431-9238-4647e4c89510"
      },
      "source": [
        "## Splitting our documents in chunks\n",
        "A second step is to split our documents (a 128-page book and 32K-character trasncript file) into smaller chunks. We use Langchain libraries here again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afaa217-7968-4e82-b791-c4ca3c099b93",
      "metadata": {
        "id": "3afaa217-7968-4e82-b791-c4ca3c099b93"
      },
      "outputs": [],
      "source": [
        "# We will use the most important library, recursive character splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5267876d-56c3-4515-950b-3622e770ca93",
      "metadata": {
        "id": "5267876d-56c3-4515-950b-3622e770ca93"
      },
      "outputs": [],
      "source": [
        "# Chunks have a character length, and an overlap values. For example (in real life, you are probably closer to 500 to 1000 and 50 to 100 respectively):\n",
        "rsplit = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=20,\n",
        "    chunk_overlap=5,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356c24ea-0d81-4cce-aff6-b1601e5515c3",
      "metadata": {
        "id": "356c24ea-0d81-4cce-aff6-b1601e5515c3"
      },
      "outputs": [],
      "source": [
        "# Let's take an example string\n",
        "text1 = 'abcdefghijklmnopqrstuvwxyz1234567890'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc10a7ed-af57-472a-b937-94cf1ed6b3e4",
      "metadata": {
        "id": "bc10a7ed-af57-472a-b937-94cf1ed6b3e4",
        "outputId": "340b94e1-6992-4013-e1d8-ec4e82d77c3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['abcdefghijklmnopqrst', 'pqrstuvwxyz123456789', '567890']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rsplit.split_text(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ea8853-b4d0-4395-ab64-e6dd854c4acc",
      "metadata": {
        "id": "d1ea8853-b4d0-4395-ab64-e6dd854c4acc"
      },
      "outputs": [],
      "source": [
        "Hamlet = \"\"\"Truly to speak, and with no addition, \\\n",
        "We go to gain a little patch of ground \\\n",
        "That hath in it no profit but the name. \\\n",
        "To pay five ducats, five, I would not farm it; \\\n",
        "Nor will it yield to Norway or the Pole \\\n",
        "A ranker rate, should it be sold in fee.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f250a326-40cf-4b99-8818-b388a489900e",
      "metadata": {
        "id": "f250a326-40cf-4b99-8818-b388a489900e",
        "outputId": "b71a0078-02ae-4aa4-f92c-6f073dba2249"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Truly to speak, and',\n",
              " 'and with no',\n",
              " 'no addition, We go',\n",
              " 'go to gain a little',\n",
              " 'patch of ground',\n",
              " 'That hath in it no',\n",
              " 'no profit but the',\n",
              " 'the name. To pay',\n",
              " 'pay five ducats,',\n",
              " 'five, I would not',\n",
              " 'not farm it; Nor',\n",
              " 'Nor will it yield',\n",
              " 'to Norway or the',\n",
              " 'the Pole A ranker',\n",
              " 'rate, should it be',\n",
              " 'be sold in fee.']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rsplit.split_text(Hamlet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192d29ed-a7c9-4958-b8f3-b1eadc2cb763",
      "metadata": {
        "id": "192d29ed-a7c9-4958-b8f3-b1eadc2cb763"
      },
      "outputs": [],
      "source": [
        "# Let's go for a more realistic chunk size\n",
        "rsplit = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ba9199-d8cb-4514-88df-fd060a97225d",
      "metadata": {
        "id": "e6ba9199-d8cb-4514-88df-fd060a97225d"
      },
      "outputs": [],
      "source": [
        "# Looking at the files, first the pdf\n",
        "rdoc1 = rsplit.split_documents(book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbdd4440-fd97-47df-8a5c-ada64dbe0a15",
      "metadata": {
        "id": "fbdd4440-fd97-47df-8a5c-ada64dbe0a15",
        "outputId": "6f4def42-0450-4262-cb52-a8bf30a2ba62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "956"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(rdoc1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d132c8-a45a-4ce3-96e5-bed41007aa30",
      "metadata": {
        "id": "58d132c8-a45a-4ce3-96e5-bed41007aa30",
        "outputId": "2b264493-d3b1-43cc-ce96-ca4b22f4f776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the splitted version has more documents (pages) than the original pdf source,\n",
        "len(book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be1616b-09a4-4d3c-9f75-7793daebf847",
      "metadata": {
        "id": "9be1616b-09a4-4d3c-9f75-7793daebf847",
        "outputId": "8439fb9c-65f6-415d-f251-142566e25bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Split 1 ---\n",
            "small and still, faintly marked with transver se stripes, and slightly flattened from the \n",
            "perfect round. But so little it was, so silvery warm--a pin's-head of li ght! It was as if it \n",
            "quivered, but really this was the telescope vi brating with the activity of the clockwork \n",
            "that kept the planet in view.     As I watched, the planet seemed to grow larger and smaller and to advance and recede, \n",
            "but that was simply that my eye was tired. Forty millions of miles it was from us--more\n",
            "\n",
            "--- Split 2 ---\n",
            "but that was simply that my eye was tired. Forty millions of miles it was from us--more \n",
            "than forty millions of miles of void. Few people realise the im- mensity of vacancy in \n",
            "which the dust of the material universe swims.  \n",
            "   Near it in the field, I re member, were three faint points of  light, three telescopic stars \n",
            "infinitely remote, and all around it was th e unfathomable darkness of empty space. You\n",
            "\n",
            "--- Split 3 ---\n",
            "infinitely remote, and all around it was th e unfathomable darkness of empty space. You \n",
            "know how that blackness looks on a frosty st arlight night. In a tele- scope it seems far \n",
            "profounder. And invisible to me because it wa s so remote and small, flying swiftly and \n",
            "steadily towards me across that incredible di stance, drawing nearer every min- ute by so \n",
            "many thousands of miles, came the Thing they  were sending us, the Thing that was to\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Printing a few splits\n",
        "for i, doc in enumerate(rdoc1[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
        "    print(f\"--- Split {i + 1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print()  # Print an empty line for better readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f840a45f-5a73-47f5-a8ed-5d049e7c9491",
      "metadata": {
        "id": "f840a45f-5a73-47f5-a8ed-5d049e7c9491",
        "outputId": "69c58a2b-0e85-482b-c89e-48523b502225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Split 1 ---\n",
            "how we actually execute analytics jobs on that data that's sitting in HDFS. So on the master node we have a new function, a new demon called the job tracker, and on the slave nodes we have a new one called the task tracker. Now let's say we have an application job that needs to communicate and analyze some data set that's sitting on the slave nodes down below. So the application job executes a Java command on the API, communicating with the name node, and then it tries to communicate down to\n",
            "\n",
            "--- Split 2 ---\n",
            "Java command on the API, communicating with the name node, and then it tries to communicate down to the task trackers below. Now one of the big differences between big data architectures and traditional data processing is that we don't try to bring all the data to one place and analyze it. What we do is we send the processing job down to the data and distribute it. You can think of it like having a lot of minions doing the work for you. One analogy might be if you had a very, very big\n",
            "\n",
            "--- Split 3 ---\n",
            "having a lot of minions doing the work for you. One analogy might be if you had a very, very big newspaper. Let's say you had a newspaper that was 10,000 pages long. And you wanted to find just one keyword in that newspaper. Well how would you do it? Well you could start on page one of that newspaper and read all the way through to page 10,000. And as you go you start to add up and count all the words that you're looking for. But that would take a very, very long time. But now what if you had a\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Splitting the trasncript of the audio file\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Step 1: Load the transcript text\n",
        "transcript_file_path = \"docs/youtube/transcript.txt\"\n",
        "with open(transcript_file_path, 'r') as f:\n",
        "    transcript_text = f.read()\n",
        "\n",
        "# Step 2: Create a Document object\n",
        "document = Document(page_content=transcript_text)\n",
        "\n",
        "# Step 3: Split the transcript into chunks\n",
        "rdoc2 = rsplit.split_documents([document])\n",
        "\n",
        "# Step 4 manually assigning the metadata to each split\n",
        "save_dir = \"docs/youtube/\"\n",
        "downloaded_file = [f for f in os.listdir(save_dir) if f.endswith('.m4a')][0]  # Assuming m4a, adjust if using mp3\n",
        "downloaded_file_path = os.path.join(save_dir, downloaded_file)\n",
        "for doc in rdoc2:\n",
        "    doc.metadata = {\"source\": \"youtube\", \"file_path\": downloaded_file_path}\n",
        "\n",
        "\n",
        "# Step 5: Print the first few splits\n",
        "for i, doc in enumerate(rdoc2[30:33]):  # Adjust the number 3 to print more or fewer splits\n",
        "    print(f\"--- Split {i + 1} ---\")\n",
        "    print(doc.page_content)\n",
        "    print()  # Print an empty line for better readability\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa78678-8df5-4967-8d44-30efb621df92",
      "metadata": {
        "id": "aaa78678-8df5-4967-8d44-30efb621df92",
        "outputId": "b06613cf-2929-4c70-a86b-3df78eab1528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata for rdoc1:\n",
            "--- Metadata for Split 1 ---\n",
            "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
            "\n",
            "--- Metadata for Split 2 ---\n",
            "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
            "\n",
            "--- Metadata for Split 3 ---\n",
            "{'source': 'docs/War-of-the-Worlds.pdf', 'page': 1}\n",
            "\n",
            "Metadata for rdoc2:\n",
            "--- Metadata for Split 1 ---\n",
            "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
            "\n",
            "--- Metadata for Split 2 ---\n",
            "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
            "\n",
            "--- Metadata for Split 3 ---\n",
            "{'source': 'youtube', 'file_path': 'docs/youtube/Big Data Architectures.m4a'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checking the metadata\n",
        "\n",
        "# Viewing metadata of the first few splits from rdoc1 (the pdf text)\n",
        "print(\"Metadata for rdoc1:\")\n",
        "for i, doc in enumerate(rdoc1[:3]):  # Adjust the number to view more or fewer splits\n",
        "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
        "    print(doc.metadata)  # Print the metadata\n",
        "    print()  # Print an empty line for better readability\n",
        "\n",
        "# Viewing metadata of the first few splits from rdoc2 (the video transcript)\n",
        "print(\"Metadata for rdoc2:\")\n",
        "for i, doc in enumerate(rdoc2[:3]):  # Adjust the number to view more or fewer splits\n",
        "    print(f\"--- Metadata for Split {i + 1} ---\")\n",
        "    print(doc.metadata)  # Print the metadata\n",
        "    print()  # Print an empty line for better readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32148e2-3c79-4eb8-943f-bb63fb4e804a",
      "metadata": {
        "id": "c32148e2-3c79-4eb8-943f-bb63fb4e804a"
      },
      "source": [
        "Recursive character splitting is a very common technique. But if you use an LLM that severly limits the number of input token (or charges you b y the token), you may want to split based on tokens instead of character sequences. This is how to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9375610b-1526-4453-9fca-46982cca06a1",
      "metadata": {
        "id": "9375610b-1526-4453-9fca-46982cca06a1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd44ea57-3fe6-4c81-8c9b-cf52c600d756",
      "metadata": {
        "id": "dd44ea57-3fe6-4c81-8c9b-cf52c600d756"
      },
      "outputs": [],
      "source": [
        "# Let's define a very small chunk and no overlap, so you can see what a chunk looks like with this method\n",
        "token_split = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04075986-8a0f-408b-a8b1-bef12f58be36",
      "metadata": {
        "id": "04075986-8a0f-408b-a8b1-bef12f58be36",
        "outputId": "b8c30a3b-c9a1-4e41-9f65-f4bcffeb4cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['T', 'ruly', ' to', ' speak', ',', ' and', ' with', ' no', ' addition', ',', ' We', ' go', ' to', ' gain', ' a', ' little', ' patch', ' of', ' ground', ' That', ' hath', ' in', ' it', ' no', ' profit', ' but', ' the', ' name', '.', ' To', ' pay', ' five', ' d', 'uc', 'ats', ',', ' five', ',', ' I', ' would', ' not', ' farm', ' it', ';', ' Nor', ' will', ' it', ' yield', ' to', ' Norway', ' or', ' the', ' Pole', ' A', ' rank', 'er', ' rate', ',', ' should', ' it', ' be', ' sold', ' in', ' fee', '.']\n"
          ]
        }
      ],
      "source": [
        "print(token_split.split_text(Hamlet))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485dca5c-472e-4cce-bda5-7a700c424572",
      "metadata": {
        "id": "485dca5c-472e-4cce-bda5-7a700c424572"
      },
      "source": [
        "## Storing in Vector Store\n",
        "The third step is to store your splits in a vector database. There are dozens of solutions. Very popular solutions for local storage include Mongodb, Chroma, Weaviate and Milvus. All large Cloud vendors (Azure, AWS etc.) offer a Cloud vectordb solution. Here we use Chroma, a locally stored, flexible popular choice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482ae5f6-5d15-48d6-b9dc-6353fd4e706d",
      "metadata": {
        "id": "482ae5f6-5d15-48d6-b9dc-6353fd4e706d"
      },
      "source": [
        "Before storing our data into the vectordb, we need to convert the text strings into vectors (embedding). We use a tokenizer compatible with the BERT model to first tokenize the text, then embed (convert to vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b832c59c-75a3-4ab6-a0a4-36a7258d91d0",
      "metadata": {
        "id": "b832c59c-75a3-4ab6-a0a4-36a7258d91d0"
      },
      "outputs": [],
      "source": [
        "# Create Ollama embeddings and vector store\n",
        "#!pip install chromadb\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "all_splits = rdoc1 + rdoc2\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc023e2-ab3e-4d50-a49a-60bb1ca5032c",
      "metadata": {
        "id": "4dc023e2-ab3e-4d50-a49a-60bb1ca5032c"
      },
      "source": [
        "What do these vectors look like? Let's play with a few examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15399743-b027-4b8b-99e9-508ad327ffe8",
      "metadata": {
        "id": "15399743-b027-4b8b-99e9-508ad327ffe8"
      },
      "outputs": [],
      "source": [
        "text1 = \"i like hotdogs\"\n",
        "text2 = \"i like sandwiches\"\n",
        "text3 = \"this is a large building\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360ae3f0-d86d-44ac-9e87-81bc0f597873",
      "metadata": {
        "id": "360ae3f0-d86d-44ac-9e87-81bc0f597873"
      },
      "outputs": [],
      "source": [
        "embedding1 = embeddings.embed_query(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dd83f4-8a8c-4054-9d0d-90fbcee01fa2",
      "metadata": {
        "id": "b8dd83f4-8a8c-4054-9d0d-90fbcee01fa2"
      },
      "outputs": [],
      "source": [
        "embedding1 = embeddings.embed_query(text1)\n",
        "embedding2 = embeddings.embed_query(text2)\n",
        "embedding3 = embeddings.embed_query(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbc5b00-af1c-45af-9bb7-2528052b012a",
      "metadata": {
        "id": "ecbc5b00-af1c-45af-9bb7-2528052b012a",
        "outputId": "27e370cc-423a-41d5-a055-d542baf73a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding1 includes 768 values\n",
            "First few values: [-0.19900505244731903, -0.022423911839723587, -3.72220778465271, -0.7225621342658997, 0.05477889999747276, 0.9443159699440002, -1.1486680507659912, 0.5535013675689697, -0.9903378486633301, -0.840915322303772]\n"
          ]
        }
      ],
      "source": [
        "# looking at the first values of the first embedding\n",
        "print(\"embedding1 includes\", len(embedding1), \"values\")\n",
        "print(\"First few values:\", embedding1[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21485d03-8e16-404e-982d-8dd292b1416c",
      "metadata": {
        "id": "21485d03-8e16-404e-982d-8dd292b1416c"
      },
      "source": [
        "How closes are these vectors from one another? There are many ways to compare them, here we use the cosine similarity method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2afb0f-944f-4b99-bfdb-fb6940a33a85",
      "metadata": {
        "id": "7e2afb0f-944f-4b99-bfdb-fb6940a33a85",
        "outputId": "3710d2aa-a7c4-4479-9ee9-27c50be1f85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity (with cos similarity) between sentence 1 and 2: 0.7179325440433726\n",
            "Similarity (with cos similarity) between sentence 1 and 3: 0.37685116184466794\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "# Step 1 : creating the normalized vectors (so the product is between 0 and 1)\n",
        "\n",
        "norm_a = np.linalg.norm(embedding1)\n",
        "norm_b = np.linalg.norm(embedding2)\n",
        "norm_c = np.linalg.norm(embedding3)\n",
        "normalized_a = embedding1 / norm_a\n",
        "normalized_b = embedding2 / norm_b\n",
        "normalized_c = embedding3 / norm_c\n",
        "\n",
        "#Step 2: comparing text1 and text 2 embeddings, then text1 and text 3 embeddings:\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
        "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
        "\n",
        "print(\"Similarity (with cos similarity) between sentence 1 and 2:\", similarity_1_2)\n",
        "print(\"Similarity (with cos similarity) between sentence 1 and 3:\", similarity_1_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e43aaa-86a7-4e60-adeb-d276fd3da258",
      "metadata": {
        "id": "f8e43aaa-86a7-4e60-adeb-d276fd3da258"
      },
      "source": [
        "Now that we have embeddings, let's store them into a Chroma database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39875f5a-4dfb-47fd-88f2-5197d15392b7",
      "metadata": {
        "id": "39875f5a-4dfb-47fd-88f2-5197d15392b7"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade langchain chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Set the environment variable to disable tokenizers parallelism and avoid warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Let's define a directory where we'll store the database beyond this notebook execution (and let's make sure it is emtpy, as I run this notebook often :))\n",
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6a4179-8f4c-4971-b201-85b08f3edd3e",
      "metadata": {
        "id": "cf6a4179-8f4c-4971-b201-85b08f3edd3e"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ef1288-a535-4058-89b8-d922781634d4",
      "metadata": {
        "id": "f5ef1288-a535-4058-89b8-d922781634d4"
      },
      "source": [
        "Now let's see if we can perform some similarity search with this database. keep in mind that we are just comparing vectors here, there is no LLM yet to smartly correlate deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5071c834-10b8-4acc-baca-fc1ae4fe022c",
      "metadata": {
        "id": "5071c834-10b8-4acc-baca-fc1ae4fe022c"
      },
      "outputs": [],
      "source": [
        "question = \"Did the spaceship come from the planet Mars?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8c9919-302b-472c-ba83-0a69d06a482f",
      "metadata": {
        "id": "4a8c9919-302b-472c-ba83-0a69d06a482f"
      },
      "outputs": [],
      "source": [
        "docs = vectordb.similarity_search(question,k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2299dabf-d085-4f22-be36-38eb8caa3996",
      "metadata": {
        "id": "2299dabf-d085-4f22-be36-38eb8caa3996",
        "outputId": "789d8013-3793-400f-da8e-2a02dc2abb2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b18a45-8cdd-4037-a229-07312f0ed401",
      "metadata": {
        "id": "65b18a45-8cdd-4037-a229-07312f0ed401",
        "outputId": "75203707-d819-4582-e703-b384070f8725"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'for the inhabitants of Mars. The immediate pr essure of necessity has brightened their \\nintellects, enlarged their pow ers, and hardened their h earts. And looking across space \\nwith instruments, and intelligences such as we have scarcely dreamed of, they see, at its \\nnearest distance only 35,000,000 of miles sunward of them, a morning star of hope, our \\nown warmer planet, green with vegetation and grey with water, w ith a cloudy atmosphere'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d124340-1dd2-4e56-908f-202fc48979f9",
      "metadata": {
        "id": "2d124340-1dd2-4e56-908f-202fc48979f9"
      },
      "outputs": [],
      "source": [
        "# Let's save the vectordb so we can use it outside of this notebook - note, this is FYI as it is automatically done with Chroma, but not with all other vectordbs!\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ef8da9-7d92-4281-a684-85b6dda2fa30",
      "metadata": {
        "id": "19ef8da9-7d92-4281-a684-85b6dda2fa30"
      },
      "source": [
        "## Retrieving with the LLM in action\n",
        "The full process consists of asking a question, retrieving the relevant information, then passing the information and the question to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbccfd8-b360-492a-9430-c3b76f048c03",
      "metadata": {
        "id": "0dbccfd8-b360-492a-9430-c3b76f048c03"
      },
      "outputs": [],
      "source": [
        "#We still need these bricks, so do not run this part of the notebook in isolation\n",
        "persist_directory = 'docs/chroma/'\n",
        "embedding = embeddings\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3867d02c-7875-41cd-a119-20e4f696b977",
      "metadata": {
        "id": "3867d02c-7875-41cd-a119-20e4f696b977",
        "outputId": "3c8d4572-cda6-4b5c-fea2-fe4ae586b03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1038\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62fe520-20ec-4f1b-b2e9-cf6bd4c85032",
      "metadata": {
        "id": "b62fe520-20ec-4f1b-b2e9-cf6bd4c85032",
        "outputId": "2b0ce808-7b6d-441a-a721-4e2cc3545dee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"Did the spaceship come from the planet Mars?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252cad83-2c21-4f36-96fa-f235bf2b3e30",
      "metadata": {
        "id": "252cad83-2c21-4f36-96fa-f235bf2b3e30"
      },
      "outputs": [],
      "source": [
        "#!pip install ollama\n",
        "#!ollama serve & ollama pull llama3 & ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e2be6d-4d44-4d5f-bc92-d1e54c47ffb3",
      "metadata": {
        "id": "c0e2be6d-4d44-4d5f-bc92-d1e54c47ffb3",
        "outputId": "3eb3206b-acac-42d8-e05c-3219b713cb1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"There is no conclusive evidence of the existence of aliens on Mars. While there have been numerous robotic missions to Mars, including rovers like NASA's Curiosity and Perseverance, as well as landers and orbiters from various countries and organizations, none of these missions have found definitive proof of extraterrestrial life on the planet.\\n\\nHowever, the search for life on Mars is an ongoing area of research and exploration. NASA's Mars 2020 rover, which includes the Perseverance rover, is equipped with instruments designed to search for signs of past or present life on Mars, such as the Sample Analysis at Mars (SAM) instrument, which can analyze Martian rocks and soil for evidence of biological activity.\\n\\nAdditionally, there are ongoing efforts to send future missions to Mars that could potentially detect signs of life, such as the European Space Agency's ExoMars rover, which is scheduled to launch in 2022. The rover will carry a suite of instruments designed to search for biosignatures, or signs of biological activity, on the Martian surface.\\n\\nIt's also worth noting that while there has been no conclusive evidence of aliens on Mars, there have been some intriguing findings and observations that have sparked scientific interest and debate. For example, in 2019, NASA's Curiosity rover discovered sedimentary rocks on Mars that had been deposited in ancient lakes and rivers, which could potentially provide a habitable environment for life.\\n\\nUltimately, while we have not found definitive proof of aliens on Mars yet, the search for life beyond Earth is an ongoing and exciting area of research that continues to advance our understanding of the universe and our place within it.\""
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using Llama3 as the LLM, and Ollama as the wrapper to interact with Llama3. Then using a test question to calidate the install.\n",
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model = \"llama3\")\n",
        "llm.invoke(\"Are there aliens on Mars?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529ca15f-1519-491d-8b1b-0c5f5be3e86b",
      "metadata": {
        "id": "529ca15f-1519-491d-8b1b-0c5f5be3e86b"
      },
      "outputs": [],
      "source": [
        "#!pip install ollama langchain beautifulsoup4 chromadb gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e4f3da-8007-4b84-b7fd-15647e5981d1",
      "metadata": {
        "id": "c5e4f3da-8007-4b84-b7fd-15647e5981d1",
        "outputId": "c8bfa354-d21e-4464-cdf9-3199306cf9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is \"almost\" the final code. You will see the final code in the last lesson of the course\n",
        "import gradio as gr\n",
        "import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# Create Ollama embeddings and vector store\n",
        "#embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "#vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "def rag_chain(question):\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    return ollama_llm(question, formatted_context)\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(question):\n",
        "    return rag_chain(question)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "  outputs=\"text\",\n",
        "  title=\"RAG with Llama3\",\n",
        "  description=\"Ask questions about the provided context\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n",
        "# example q: did the aliens eventually go on to land on Venus?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369fc91c-e24b-4638-86d2-37627a33af44",
      "metadata": {
        "id": "369fc91c-e24b-4638-86d2-37627a33af44"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0670c45b-f553-48cf-b577-f443459f33a8",
      "metadata": {
        "id": "0670c45b-f553-48cf-b577-f443459f33a8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}